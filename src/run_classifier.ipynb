{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import argparse\n",
    "from features import extract_features_omniglot, extract_features_mini_imagenet\n",
    "from inference import infer_classifier\n",
    "from utilities import sample_normal, multinoulli_log_density, print_and_log, get_log_files\n",
    "from data import get_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hard-Code Once Command-Line Args\n",
    "\n",
    "# Dataset to use\n",
    "dataset = 'Omniglot'\n",
    "\n",
    "# Whether to run traing only, testing only, or both training and testing.\n",
    "mode = 'train_test'\n",
    "\n",
    "# Size of the feature extractor output.\n",
    "d_theta = 256\n",
    "\n",
    "# Number of training examples.\n",
    "shot = 5\n",
    "\n",
    "# Number of classes.\n",
    "way = 5\n",
    "\n",
    "# Shot to be used at evaluation time. If not specified 'shot' will be used.\n",
    "test_shot = None\n",
    "\n",
    "# Way to be used at evaluation time. If not specified 'way' will be used.\n",
    "test_way = None\n",
    "\n",
    "# Number of tasks per batch.\n",
    "tasks_per_batch = 16\n",
    "\n",
    "# Number of samples from q.\n",
    "samples = 10\n",
    "\n",
    "# Learning rate.\n",
    "learning_rate = 1e-4\n",
    "\n",
    "# Number of training iterations.\n",
    "iterations = 800\n",
    "\n",
    "# Directory to save trained models.\n",
    "checkpoint_dir = './checkpoint'\n",
    "\n",
    "# Dropout keep probability.\n",
    "dropout = 0.9\n",
    "\n",
    "# Model to load and test.\n",
    "test_model_path = None\n",
    "\n",
    "# Frequency of summary results (in iterations).\n",
    "print_freq = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "logfile, checkpoint_path_validation, checkpoint_path_final = get_log_files(checkpoint_dir)\n",
    "\n",
    "print_and_log(logfile, \"Options:\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load training and eval data\n",
    "data = get_data(dataset)\n",
    "# <omniglot.OmniglotData object at 0x7fb091f4eed0>\n",
    "\n",
    "# set the feature extractor based on the dataset\n",
    "feature_extractor_fn = extract_features_mini_imagenet\n",
    "if dataset == \"Omniglot\":\n",
    "    feature_extractor_fn = extract_features_omniglot\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluation samples\n",
    "eval_samples_train = 15\n",
    "eval_samples_test = shot\n",
    "\n",
    "# testing parameters\n",
    "test_iterations = 600\n",
    "test_args_per_batch = 1  # always use a batch size of 1 for testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf placeholders\n",
    "train_images = tf.placeholder(tf.float32, [None,  # tasks per batch\n",
    "                                        None,  # shot\n",
    "                                        data.get_image_height(),\n",
    "                                        data.get_image_width(),\n",
    "                                        data.get_image_channels()],\n",
    "                            name='train_images')\n",
    "test_images = tf.placeholder(tf.float32, [None,  # tasks per batch\n",
    "                                        None,  # num test images\n",
    "                                        data.get_image_height(),\n",
    "                                        data.get_image_width(),\n",
    "                                        data.get_image_channels()],\n",
    "                            name='test_images')\n",
    "train_labels = tf.placeholder(tf.float32, [None,  # tasks per batch\n",
    "                                        None,  # shot\n",
    "                                        way],\n",
    "                            name='train_labels')\n",
    "test_labels = tf.placeholder(tf.float32, [None,  # tasks per batch\n",
    "                                        None,  # num test images\n",
    "                                        way],\n",
    "                            name='test_labels')\n",
    "\n",
    "dropout_keep_prob = tf.placeholder(tf.float32, [], name='dropout_keep_prob')\n",
    "L = tf.constant(samples, dtype=tf.float32, name=\"num_samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Relevant computations for a single task\n",
    "def evaluate_task(inputs):\n",
    "    train_inputs, train_outputs, test_inputs, test_outputs = inputs\n",
    "    with tf.variable_scope('shared_features'):\n",
    "        # extract features from train and test data\n",
    "        features_train = feature_extractor_fn(images=train_inputs,\n",
    "                                                output_size=d_theta,\n",
    "                                                use_batch_norm=True,\n",
    "                                                dropout_keep_prob=dropout_keep_prob)\n",
    "        features_test = feature_extractor_fn(images=test_inputs,\n",
    "                                                output_size=d_theta,\n",
    "                                                use_batch_norm=True,\n",
    "                                                dropout_keep_prob=dropout_keep_prob)\n",
    "    # Infer classification layer from q\n",
    "    with tf.variable_scope('classifier'):\n",
    "        classifier = infer_classifier(features_train, train_outputs, d_theta, way)\n",
    "\n",
    "    # Local reparameterization trick\n",
    "    # Compute parameters of q distribution over logits\n",
    "    weight_mean, bias_mean = classifier['weight_mean'], classifier['bias_mean']\n",
    "    weight_log_variance, bias_log_variance = classifier['weight_log_variance'], classifier['bias_log_variance']\n",
    "    logits_mean_test = tf.matmul(features_test, weight_mean) + bias_mean\n",
    "    logits_log_var_test =\\\n",
    "        tf.log(tf.matmul(features_test ** 2, tf.exp(weight_log_variance)) + tf.exp(bias_log_variance))\n",
    "    logits_sample_test = sample_normal(logits_mean_test, logits_log_var_test, samples)\n",
    "    test_labels_tiled = tf.tile(tf.expand_dims(test_outputs, 0), [samples, 1, 1])\n",
    "    task_log_py = multinoulli_log_density(inputs=test_labels_tiled, logits=logits_sample_test)\n",
    "    averaged_predictions = tf.reduce_logsumexp(logits_sample_test, axis=0) - tf.log(L)\n",
    "    task_accuracy = tf.reduce_mean(tf.cast(tf.equal(tf.argmax(test_outputs, axis=-1),\n",
    "                                                    tf.argmax(averaged_predictions, axis=-1)), tf.float32))\n",
    "    task_score = tf.reduce_logsumexp(task_log_py, axis=0) - tf.log(L)\n",
    "    task_loss = -tf.reduce_mean(task_score, axis=0)\n",
    "\n",
    "    return [task_loss, task_accuracy]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf mapping of batch to evaluation function\n",
    "batch_output = tf.map_fn(fn=evaluate_task,\n",
    "                            elems=(train_images, train_labels, test_images, test_labels),\n",
    "                            dtype=[tf.float32, tf.float32],\n",
    "                            parallel_iterations=tasks_per_batch)\n",
    "\n",
    "# average all values across batch\n",
    "batch_losses, batch_accuracies = batch_output\n",
    "loss = tf.reduce_mean(batch_losses)\n",
    "accuracy = tf.reduce_mean(batch_accuracies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    saver = tf.train.Saver()\n",
    "\n",
    "    if mode == 'train' or mode == 'train_test':\n",
    "        # train the model\n",
    "        optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "        train_step = optimizer.minimize(loss)\n",
    "\n",
    "        validation_batches = 200\n",
    "        iteration = 0\n",
    "        best_validation_accuracy = 0.0\n",
    "        train_iteration_accuracy = []\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        # Main training loop\n",
    "        while iteration < iterations:\n",
    "            \n",
    "            train_inputs, test_inputs, train_outputs, test_outputs = \\\n",
    "                data.get_batch('train', tasks_per_batch, shot, way, eval_samples_train)\n",
    "            feed_dict = {train_images: train_inputs, test_images: test_inputs,\n",
    "                            train_labels: train_outputs, test_labels: test_outputs,\n",
    "                            dropout_keep_prob: dropout}\n",
    "            \n",
    "            # Train for One Iteration\n",
    "            _, iteration_loss, iteration_accuracy = sess.run([train_step, loss, accuracy], feed_dict)\n",
    "            train_iteration_accuracy.append(iteration_accuracy)\n",
    "\n",
    "            # Print Debugging Infrmation to Console and Log Ooutput\n",
    "            # Execute Validation Test\n",
    "            if (iteration > 0) and (iteration % print_freq == 0):\n",
    "                # compute accuracy on validation set\n",
    "                validation_iteration_accuracy = []\n",
    "                validation_iteration = 0\n",
    "                while validation_iteration < validation_batches:\n",
    "                    \n",
    "                    train_inputs, test_inputs, train_outputs, test_outputs = \\\n",
    "                        data.get_batch('validation', tasks_per_batch, shot, way, eval_samples_test)\n",
    "                    feed_dict = {train_images: train_inputs, test_images: test_inputs,\n",
    "                                    train_labels: train_outputs, test_labels: test_outputs,\n",
    "                                    dropout_keep_prob: 1.0}\n",
    "\n",
    "                    iteration_accuracy = sess.run(accuracy, feed_dict)\n",
    "                    validation_iteration_accuracy.append(iteration_accuracy)\n",
    "                    validation_iteration += 1\n",
    "                \n",
    "                validation_accuracy = np.array(validation_iteration_accuracy).mean()\n",
    "                train_accuracy = np.array(train_iteration_accuracy).mean()\n",
    "\n",
    "                # save checkpoint if validation is the best so far\n",
    "                if validation_accuracy > best_validation_accuracy:\n",
    "                    best_validation_accuracy = validation_accuracy\n",
    "                    saver.save(sess=sess, save_path=checkpoint_path_validation)\n",
    "\n",
    "                print_and_log(logfile, 'Iteration: {}, Loss: {:5.3f}, Train-Acc: {:5.3f}, Val-Acc: {:5.3f}'\n",
    "                    .format(iteration, iteration_loss, train_accuracy, validation_accuracy))\n",
    "                train_iteration_accuracy = []\n",
    "\n",
    "            iteration += 1\n",
    "        # save the checkpoint from the final epoch\n",
    "        saver.save(sess, save_path=checkpoint_path_final)\n",
    "        print_and_log(logfile, 'Fully-trained model saved to: {}'.format(checkpoint_path_final))\n",
    "        print_and_log(logfile, 'Best validation accuracy: {:5.3f}'.format(best_validation_accuracy))\n",
    "        print_and_log(logfile, 'Best validation model saved to: {}'.format(checkpoint_path_validation))\n",
    "    \n",
    "    def test_model(model_path, load=True):\n",
    "        if load:\n",
    "            saver.restore(sess, save_path=model_path)\n",
    "        test_iteration = 0\n",
    "        test_iteration_accuracy = []\n",
    "        \n",
    "        # Main Test Loop\n",
    "        while test_iteration < test_iterations:\n",
    "            \n",
    "            train_inputs, test_inputs, train_outputs, test_outputs = \\\n",
    "                                    data.get_batch('test', test_args_per_batch, test_shot, test_way,\n",
    "                                                    eval_samples_test)\n",
    "            feedDict = {train_images: train_inputs, test_images: test_inputs,\n",
    "                        train_labels: train_outputs, test_labels: test_outputs,\n",
    "                        dropout_keep_prob: 1.0}\n",
    "\n",
    "            iter_acc = sess.run(accuracy, feedDict)\n",
    "            test_iteration_accuracy.append(iter_acc)\n",
    "            test_iteration += 1\n",
    "\n",
    "        test_accuracy = np.array(test_iteration_accuracy).mean() * 100.0\n",
    "        confidence_interval_95 = (196.0 * np.array(test_iteration_accuracy).std()) / np.sqrt(len(test_iteration_accuracy))\n",
    "        \n",
    "        print_and_log(logfile, 'Held out accuracy: {0:5.3f} +/- {1:5.3f} on {2:}'\n",
    "                        .format(test_accuracy, confidence_interval_95, model_path))\n",
    "\n",
    "    if mode == 'train_test':\n",
    "        print_and_log(logfile, 'Train Shot: {0:d}, Train Way: {1:d}, Test Shot {2:d}, Test Way {3:d}'\n",
    "                        .format(shot, way, test_shot, test_way))\n",
    "        # test the model on the final trained model\n",
    "        # no need to load the model, it was just trained\n",
    "        test_model(checkpoint_path_final, load=False)\n",
    "\n",
    "        # test the model on the best validation checkpoint so far\n",
    "        test_model(checkpoint_path_validation)\n",
    "\n",
    "    if mode == 'test':\n",
    "        test_model(test_model_path)\n",
    "\n",
    "logfile.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.13 ('flow1')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "02b713c2907d9ec3e75f9997c30821fa46485eb357aca116b67f32755273915a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
